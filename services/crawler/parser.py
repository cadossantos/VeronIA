import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from services.crawler.models import SmartWikiPage

class WikiParser:
    """Respons치vel por extrair o conte칰do e metadados de uma p치gina da SmartWiki."""

    def __init__(self, base_url: str):
        self.base_url = base_url

    def parse(self, url: str, html: str) -> SmartWikiPage:
        soup = BeautifulSoup(html, "lxml")

        # Extrai t칤tulo da p치gina
        title_tag = soup.find("h1")
        title = title_tag.text.strip() if title_tag else "Sem t칤tulo"

        # Localiza o conte칰do principal da p치gina
        content_div = soup.find("div", {"id": "bodyContent"})
        raw_content = content_div.get_text(separator="\n").strip() if content_div else "Sem conte칰do"
        content = re.sub(r'\t+', ' ', raw_content)       # Remove tabs
        content = re.sub(r'\n{2,}', '\n', content)        # Reduz m칰ltiplas quebras de linha
        content = re.sub(r' {2,}', ' ', content)          # Reduz m칰ltiplos espa칞os

        # Extrai links internos
        raw_links = content_div.find_all("a", href=True) if content_div else []
        links = [
            urljoin(self.base_url, a["href"])
            for a in raw_links
            if a["href"].startswith("/wiki/") and ":" not in a["href"]
        ]

        # Extrai imagens (atributos src absolutos)
        raw_images = content_div.find_all("img", src=True) if content_div else []
        images = [
            urljoin(self.base_url, img["src"])
            for img in raw_images
        ]

        # Adiciona campo opcional de imagens ao modelo
        page = SmartWikiPage(
            url=url,
            title=title,
            content=content,
            links=links,
        )

        # 游댢 Adiciona dinamicamente se quiser j치 suportar isso
        setattr(page, "images", images)

        return page
