# Inspe√ß√£o de Resqu√≠cios e Gaps da Funcionalidade RAG

A seguir vai um levantamento dos principais ‚Äúresqu√≠cios‚Äù da antiga estrutura _smartwiki_ e dos gaps de integra√ß√£o que ainda existem para que a funcionalidade RAG (Retrieval‚ÄëAugmented Generation) fique totalmente operacional na aplica√ß√£o principal.

---

## 1) Limpeza / remo√ß√£o / reorganiza√ß√£o dos artefatos do antigo ‚Äúsmartwiki‚Äù

No diret√≥rio `smartwiki/` (o MVP isolado que foi criado originalmente) ainda existem diversos arquivos e pastas duplicados na raiz do projeto ou n√£o mais utilizados:

- **Reposit√≥rio Git isolado**  
  H√° um sub‚Äë`.git` completo dentro de `smartwiki/`, indicando que era um reposit√≥rio independente.  
  ```text
  smartwiki/.git/HEAD
  smartwiki/.git/config
  ...
  ```

- **Ambiente virtual interno**  
  A pasta `.venv/` cont√©m todo o ambiente Python isolado, mas n√£o √© usado pelo projeto principal.  
  ```text
  smartwiki/.venv/bin/‚Ä¶
  smartwiki/.venv/lib/‚Ä¶
  ```

- **Gerenciamento pr√≥prio de depend√™ncias**  
  H√° um `pyproject.toml` dentro de `smartwiki/` com configura√ß√µes legadas que conflitam com o da raiz.

- **C√≥digo duplicado ou n√£o consumido**  
  As implementa√ß√µes de RAG e crawling originais (em `smartwiki/agents/`, `smartwiki/crawler/`) n√£o s√£o referenciadas pelos servi√ßos atuais na raiz.

- **Pasta de testes isolada**  
  Existem testes de ingest√£o e consulta em `smartwiki/tests/` que n√£o s√£o executados junto ao restante do projeto.

**Conclus√£o (parte‚ÄØ1):**  
Remover completamente o diret√≥rio `smartwiki/` (incluindo `.git`, `.venv`, testes e crawler legado) e, caso haja m√≥dulos √∫teis, extrair apenas o que for reaproveit√°vel para `services/` ou `utils/`.

---

## 2) Gaps de integra√ß√£o e funcionalidades RAG ainda n√£o implementadas

Mesmo com o core de RAG presente (`agents/rag_agent.py` e `services/rag_service.py`), h√° lacunas no fluxo que precisam ser fechadas para um suporte completo:

- **Workflow de indexa√ß√£o**  
  O bot√£o ‚ÄúIndexar documentos‚Äù na aba de RAG do sidebar n√£o chama efetivamente o servi√ßo de ingest√£o (`services.ingest_service.ingest`) nem gera chunks.

- **Persist√™ncia de arquivos**  
  Os arquivos enviados pelo uploader de RAG permanecem em mem√≥ria e n√£o s√£o salvos em disco (`db/pages`), portanto o ingest_service n√£o os encontra.

- **Par√¢metros hard‚Äëcoded**  
  Valores como `VECTOR_STORE_DIR`, `EMBEDDING_MODEL`, al√©m de `chunk_size` e `overlap`, est√£o fixos nos m√≥dulos em vez de serem extra√≠dos do `session_state` configurado pela UI.

- **Sele√ß√£o est√°tica de bases**  
  A lista de bases dispon√≠veis (ex: ‚ÄòDocumentos Gerais‚Äô, ‚ÄòBase T√©cnica‚Äô) √© est√°tica e n√£o mapeia para collections reais do ChromaDB.

- **Bot√£o ‚ÄúAtualizar Base‚Äù sem l√≥gica**  
  N√£o existe chamada para reindexa√ß√£o no clique desse bot√£o; apenas exibe uma mensagem de sucesso.

- **M√©tricas placeholders**  
  A se√ß√£o de m√©tricas de RAG exibe valores est√°ticos em vez de contadores reais de documentos e chunks.

### Pr√≥ximos passos sugeridos

1. **Persistir uploads** em `db/pages` e invocar `services.ingest_service.ingest` com par√¢metros da UI (chunk_size, overlap, modelo_embedding).  
2. **Consumir par√¢metros din√¢micos** em `ingest_service` e `RagQueryEngine`, removendo valores hard‚Äëcoded.  
3. **Vincular sele√ß√£o de base** ao diret√≥rio/collection correto de ChromaDB.  
4. **Exibir feedback real** de quantos documentos/chunks foram indexados e atualizar m√©tricas de RAG dinamicamente.  
5. **Remover o diret√≥rio `smartwiki/`** e atualizar a documenta√ß√£o para refletir apenas a arquitetura atual.  

---

## Resumo

- Limpeza geral dos artefatos legados do MVP isolado _smartwiki_.  
- Implementa√ß√£o completa do fluxo end‚Äëto‚Äëend de RAG: indexa√ß√£o, persist√™ncia, consulta e m√©tricas.  
- Atualiza√ß√£o da documenta√ß√£o e remo√ß√£o de duplicidades.




# codex

A seguir um plano de alto n√≠vel para dar ao usu√°rio a capacidade de "apontar" novos links da SmartWiki via UI, gerar dinamicamente bases de conhecimento separadas (mais uma base "Todos") e unificar o logging.

## 1. Reaproveitamento do Crawler e unifica√ß√£o de logs

Extrair o crawler para "services/crawler/".
- Mova todos os m√≥dulos de smartwiki/crawler/ (fetcher, parser, storage, models, category_fetcher, config) para services/crawler/.
- Ajuste imports em cada arquivo (de from crawler.xxx import ‚Ä¶ para from services.crawler.xxx import ‚Ä¶).

Unificar logger.
- Centralize o logger legad√°rio de smartwiki/utils/logger.py em um √∫nico utils/logger.py.
- Substitua todos os prints no crawler, ingest√£o e RagQueryEngine por chamadas ao logger centralizado (e.g. logger.info(), logger.error()).

## 2. Fluxo de UI para "adicionar links" e cria√ß√£o de bases

### 2.1. Barra lateral (sidebar) ‚Äì se√ß√£o "Documentos para raspagem"

1. Adicionar expander "üìÑ Scramping".
2. Dentro dele, um campo de texto para url , um para nome da base de conhecimento + bot√£o "‚ûï Iniciar Scramping".

```python
with tab.expander("üìÑ Scramping", expanded=False):
    link = st.text_input("URL da p√°gina / categoria SmartWiki", key="new_smartwiki_link")
    if st.button("‚ûï Adicionar Link"):
        # 1) Persistir esse link como √© feito no arquivo smartwiki/utils/main..py
        # 2) Feedback ao usu√°rio: "Link salvo"
```

3. Listagem din√¢mica dos links j√° cadastrados, com op√ß√£o de remover ou editar.

### 2.2. Persist√™ncia dos links e gera√ß√£o de bases

1.
Armazenar os links num JSON de configura√ß√£o (e.g. db/smartwiki_links.json), no formato:

```json
{
  "bases": {
    "Todos": [],
    "base1": ["https://smartwiki/...", "..."],
    "base2": [        ]
  }
}
```

2.
Sempre que um link for adicionado/removido, regrave o JSON e exiba mensagem de sucesso.

3.
Op√ß√£o "Todos"
- Initialize a estrutura com a base "Todos" (que agrega todos os links).
- Ao criar nova base (p.ex. "base3"), adicione-a tamb√©m √† "Todos" para efeito de raspagem completa.

### 2.3. Indexa√ß√£o (ingest) por base

1. No expander "üìÑ Documentos para indexa√ß√£o", substitua o uploader est√°tico por:
   - Selectbox de bases dispon√≠veis (l√™ as chaves do JSON db/smartwiki_links.json), incluindo "Todos".
   - Bot√£o "üì• Indexar base selecionada".
2. Ao clicar em "Indexar base selecionada":
   - Carregue a lista de URLs daquela base.
   - Execute o crawler (services/crawler) para cada URL, gerando um JSON por p√°gina em db/pages/<nome da base>/‚Ä¶.
   - Em seguida, invoque o servi√ßo ingest(data_dir, vector_store_dir, collection_name, embeddings, chunk_size, overlap), usando par√¢metros de chunk_size, overlap e embedding model vindos do session_state.

## 3. Par√¢metros din√¢micos j√° dispon√≠veis na UI

| Par√¢metro UI         | Onde consumir                                                  |
|----------------------|---------------------------------------------------------------|
| Modelo de embedding  | st.session_state['modelo_embedding'] ‚Üí ingest_service        |
| chunk_size, overlap  | st.session_state['chunk_size'], ['chunk_overlap'] ‚Üí ingest_service |
| Relev√¢ncia_threshold | pode ser usado para filtragem no retriever (opcional)        |

Na implementa√ß√£o do ingest(data_dir, vector_store_dir, collection_name, embeddings, chunk_size, overlap), passe estes valores em vez de usar constantes.

## 4. M√©tricas e feedback

1. Ap√≥s cada indexa√ß√£o, retorne de ingest():
   - Quantidade de documentos raspados
   - Quantidade de chunks gerados
2. Atualize a se√ß√£o "üìä M√©tricas" do sidebar (expander RAG) para exibir esses valores reais em vez dos placeholders.

## 5. Resumo do Roadmap

1. Mover e adaptar o crawler para services/crawler/ e unificar logger em utils/logger.py.
2. Persistir links da SmartWiki via UI num JSON (incluindo base "Todos").
3. Indexar bases individualmente: crawler ‚Üí JSON pages ‚Üí ingest ‚Üí ChromaDB.
4. Consumir par√¢metros din√¢micos de embedding, chunk_size e overlap.
5. Exibir m√©tricas reais de documentos/chunks ap√≥s indexa√ß√£o.

Com esse plano teremos um pipeline totalmente integrado: o usu√°rio adiciona links, cria-se uma base, faz raspagem + ingest√£o parametrizada e, finalmente, consulta RAG com feedback de m√©tricas. Depois de aprovado, podemos partir para a implementa√ß√£o.